<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>WICWIU: WICWIU_src/Optimizer/AdagradOptimizer.hpp Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">WICWIU
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('_adagrad_optimizer_8hpp_source.html','');});
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">AdagradOptimizer.hpp</div>  </div>
</div><!--header-->
<div class="contents">
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;<span class="preprocessor">#ifndef ADAGRADOPTIMIZER_H_</span></div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;<span class="preprocessor">#define ADAGRADOPTIMIZER_H_    value</span></div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;<span class="preprocessor">#include &quot;../Optimizer.hpp&quot;</span></div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;</div><div class="line"><a name="l00007"></a><span class="lineno"><a class="line" href="class_adagrad_optimizer.html">    7</a></span>&#160;<span class="keyword">template</span>&lt;<span class="keyword">typename</span> DTYPE&gt; <span class="keyword">class </span><a class="code" href="class_adagrad_optimizer.html">AdagradOptimizer</a> : <span class="keyword">public</span> <a class="code" href="class_optimizer.html">Optimizer</a>&lt;DTYPE&gt;{</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;<span class="keyword">private</span>:</div><div class="line"><a name="l00009"></a><span class="lineno"><a class="line" href="class_adagrad_optimizer.html#a693d28d980ee2772a62006137e96f4bd">    9</a></span>&#160;    <a class="code" href="class_container.html">Container&lt;Operator&lt;DTYPE&gt;</a> *&gt; *<a class="code" href="class_adagrad_optimizer.html#a693d28d980ee2772a62006137e96f4bd">m_ppParameter</a>;</div><div class="line"><a name="l00011"></a><span class="lineno"><a class="line" href="class_adagrad_optimizer.html#a99aa9af52b00e6b72774d9e602b9cfbc">   11</a></span>&#160;    <a class="code" href="class_container.html">Container&lt;Tensor&lt;DTYPE&gt;</a> *&gt; *<a class="code" href="class_adagrad_optimizer.html#a99aa9af52b00e6b72774d9e602b9cfbc">m_aaGradientSquared</a>;</div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;</div><div class="line"><a name="l00014"></a><span class="lineno"><a class="line" href="class_adagrad_optimizer.html#a4d44b0edfcacb2b53c137a364a22eeca">   14</a></span>&#160;    <span class="keywordtype">int</span> <a class="code" href="class_adagrad_optimizer.html#a4d44b0edfcacb2b53c137a364a22eeca">m_numOfParameter</a>;</div><div class="line"><a name="l00016"></a><span class="lineno"><a class="line" href="class_adagrad_optimizer.html#a24a936788c47e2f08a72d6862a312798">   16</a></span>&#160;    <span class="keywordtype">float</span> <a class="code" href="class_adagrad_optimizer.html#a24a936788c47e2f08a72d6862a312798">m_epsilon</a>;</div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;</div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;<span class="keyword">public</span>:</div><div class="line"><a name="l00030"></a><span class="lineno"><a class="line" href="class_adagrad_optimizer.html#ab0087e92268ee3700a50f7e545fb5048">   30</a></span>&#160;    <a class="code" href="class_adagrad_optimizer.html#ab0087e92268ee3700a50f7e545fb5048">AdagradOptimizer</a>(<a class="code" href="class_container.html">Container</a>&lt;<a class="code" href="class_operator.html">Operator&lt;DTYPE&gt;</a> *&gt; *pParameterContainer, <span class="keywordtype">float</span> pLearningRate, OptimizeDirection pOptimizeDirection) : <a class="code" href="class_optimizer.html">Optimizer</a>&lt;DTYPE&gt;(pParameterContainer, pLearningRate, pOptimizeDirection) {</div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;<span class="preprocessor">        #ifdef __DEBUG__</span></div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;        std::cout &lt;&lt; <span class="stringliteral">&quot;AdagradOptimizer::AdagradOptimizer(LossFunction&lt;DTYPE&gt; *, float, OptimizeDirection)&quot;</span> &lt;&lt; <span class="charliteral">&#39;\n&#39;</span>;</div><div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;<span class="preprocessor">        #endif  // __DEBUG__</span></div><div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;        <a class="code" href="class_adagrad_optimizer.html#a693d28d980ee2772a62006137e96f4bd">m_ppParameter</a>       = NULL;</div><div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;        <a class="code" href="class_adagrad_optimizer.html#a99aa9af52b00e6b72774d9e602b9cfbc">m_aaGradientSquared</a> = NULL;</div><div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;</div><div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;        <a class="code" href="class_adagrad_optimizer.html#a4d44b0edfcacb2b53c137a364a22eeca">m_numOfParameter</a> = 0;</div><div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;        <a class="code" href="class_adagrad_optimizer.html#a24a936788c47e2f08a72d6862a312798">m_epsilon</a>        = 0.f;</div><div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;</div><div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;        Alloc();</div><div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;    }</div><div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;</div><div class="line"><a name="l00054"></a><span class="lineno"><a class="line" href="class_adagrad_optimizer.html#a4eece73c46a11c319a93ca89a589d730">   54</a></span>&#160;    <a class="code" href="class_adagrad_optimizer.html#a4eece73c46a11c319a93ca89a589d730">AdagradOptimizer</a>(<a class="code" href="class_container.html">Container</a>&lt;<a class="code" href="class_operator.html">Operator&lt;DTYPE&gt;</a> *&gt; *pParameterContainer, <span class="keywordtype">float</span> pLearningRate, <span class="keywordtype">float</span> epsilon, OptimizeDirection pOptimizeDirection) : <a class="code" href="class_optimizer.html">Optimizer</a>&lt;DTYPE&gt;(pParameterContainer, pLearningRate, pOptimizeDirection) {</div><div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;<span class="preprocessor">        #ifdef __DEBUG__</span></div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;        std::cout &lt;&lt; <span class="stringliteral">&quot;AdagradOptimizer::AdagradOptimizer(LossFunction&lt;DTYPE&gt; *, float, OptimizeDirection)&quot;</span> &lt;&lt; <span class="charliteral">&#39;\n&#39;</span>;</div><div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;<span class="preprocessor">        #endif  // __DEBUG__</span></div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;        <a class="code" href="class_adagrad_optimizer.html#a693d28d980ee2772a62006137e96f4bd">m_ppParameter</a>       = NULL;</div><div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;        <a class="code" href="class_adagrad_optimizer.html#a99aa9af52b00e6b72774d9e602b9cfbc">m_aaGradientSquared</a> = NULL;</div><div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;</div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;        <a class="code" href="class_adagrad_optimizer.html#a4d44b0edfcacb2b53c137a364a22eeca">m_numOfParameter</a> = 0;</div><div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;        <a class="code" href="class_adagrad_optimizer.html#a24a936788c47e2f08a72d6862a312798">m_epsilon</a>        = 0.f;</div><div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;</div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;        Alloc(epsilon);</div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;    }</div><div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;</div><div class="line"><a name="l00071"></a><span class="lineno"><a class="line" href="class_adagrad_optimizer.html#a9b28f6ca7f58bd151f44e5043d2735b2">   71</a></span>&#160;    <a class="code" href="class_adagrad_optimizer.html#a9b28f6ca7f58bd151f44e5043d2735b2">~AdagradOptimizer</a>() {</div><div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;<span class="preprocessor">        #ifdef __DEBUG__</span></div><div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;        std::cout &lt;&lt; <span class="stringliteral">&quot;AdagradOptimizer::~AdagradOptimizer()&quot;</span> &lt;&lt; <span class="charliteral">&#39;\n&#39;</span>;</div><div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;<span class="preprocessor">        #endif  // __DEBUG__</span></div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;        <a class="code" href="class_adagrad_optimizer.html#aa4c51cf728ca90d2aa6bc71d1c4f22d6">Delete</a>();</div><div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;    }</div><div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;</div><div class="line"><a name="l00083"></a><span class="lineno"><a class="line" href="class_adagrad_optimizer.html#aa4c51cf728ca90d2aa6bc71d1c4f22d6">   83</a></span>&#160;    <span class="keywordtype">void</span> <a class="code" href="class_adagrad_optimizer.html#aa4c51cf728ca90d2aa6bc71d1c4f22d6">Delete</a>() {</div><div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;        <span class="keywordflow">if</span> (<a class="code" href="class_adagrad_optimizer.html#a99aa9af52b00e6b72774d9e602b9cfbc">m_aaGradientSquared</a>) {</div><div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;            <span class="keyword">delete</span> <a class="code" href="class_adagrad_optimizer.html#a99aa9af52b00e6b72774d9e602b9cfbc">m_aaGradientSquared</a>;</div><div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;            <a class="code" href="class_adagrad_optimizer.html#a99aa9af52b00e6b72774d9e602b9cfbc">m_aaGradientSquared</a> = NULL;</div><div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;        }</div><div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;    }</div><div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;</div><div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;    <span class="keywordtype">int</span> Alloc() {</div><div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;        <a class="code" href="class_adagrad_optimizer.html#a693d28d980ee2772a62006137e96f4bd">m_ppParameter</a>    = this-&gt;GetTrainableTensor();</div><div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;        <a class="code" href="class_adagrad_optimizer.html#a4d44b0edfcacb2b53c137a364a22eeca">m_numOfParameter</a> = this-&gt;GetTrainableTensorDegree();</div><div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;</div><div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;        <span class="keywordflow">return</span> TRUE;</div><div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;    }</div><div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;</div><div class="line"><a name="l00106"></a><span class="lineno"><a class="line" href="class_adagrad_optimizer.html#ae8b0d226a29ac5716fbffe0add5000ed">  106</a></span>&#160;    <span class="keywordtype">int</span> <a class="code" href="class_adagrad_optimizer.html#ae8b0d226a29ac5716fbffe0add5000ed">Alloc</a>(<span class="keywordtype">float</span> epsilon) {</div><div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;        Alloc();</div><div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;</div><div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;        <a class="code" href="class_adagrad_optimizer.html#a99aa9af52b00e6b72774d9e602b9cfbc">m_aaGradientSquared</a> = <span class="keyword">new</span> <a class="code" href="class_container.html">Container&lt;Tensor&lt;DTYPE&gt;</a> *&gt;();</div><div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;</div><div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;        <a class="code" href="class_shape.html">Shape</a> *pParameterGradShape = NULL;</div><div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;</div><div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;        <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; <a class="code" href="class_adagrad_optimizer.html#a4d44b0edfcacb2b53c137a364a22eeca">m_numOfParameter</a>; i++) {</div><div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;            pParameterGradShape = (*m_ppParameter)[i]-&gt;GetGradient()-&gt;GetShape();</div><div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;            <a class="code" href="class_adagrad_optimizer.html#a99aa9af52b00e6b72774d9e602b9cfbc">m_aaGradientSquared</a>-&gt;<a class="code" href="class_container.html#ac875fe061a6d4c1137def5251eaef95a">Push</a>(<span class="keyword">new</span> <a class="code" href="class_tensor.html">Tensor&lt;DTYPE&gt;</a>(<span class="keyword">new</span> <a class="code" href="class_shape.html">Shape</a>(pParameterGradShape)));</div><div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;            pParameterGradShape = NULL;</div><div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;        }</div><div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;</div><div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;        <a class="code" href="class_adagrad_optimizer.html#a24a936788c47e2f08a72d6862a312798">m_epsilon</a> = epsilon;</div><div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;</div><div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;        <span class="keywordflow">return</span> TRUE;</div><div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;    }</div><div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160;</div><div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;    <span class="keywordtype">void</span> InitializeAttributeForGPU(<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> idOfDevice) {</div><div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;        <span class="keywordflow">if</span> (<a class="code" href="class_adagrad_optimizer.html#a24a936788c47e2f08a72d6862a312798">m_epsilon</a> != 0.f) {</div><div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;            <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; <a class="code" href="class_adagrad_optimizer.html#a4d44b0edfcacb2b53c137a364a22eeca">m_numOfParameter</a>; i++) {</div><div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;                (*m_aaGradientSquared)[i]-&gt;SetDeviceGPU(idOfDevice);</div><div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;            }</div><div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;        }</div><div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;    }</div><div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;</div><div class="line"><a name="l00138"></a><span class="lineno"><a class="line" href="class_adagrad_optimizer.html#a770179d3d869898282bb7397e93003fb">  138</a></span>&#160;    <span class="keyword">virtual</span> <span class="keywordtype">int</span> <a class="code" href="class_adagrad_optimizer.html#a770179d3d869898282bb7397e93003fb">UpdateParameter</a>() {</div><div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;        <span class="keywordflow">if</span> (<a class="code" href="class_adagrad_optimizer.html#a24a936788c47e2f08a72d6862a312798">m_epsilon</a> == 0.f) {</div><div class="line"><a name="l00140"></a><span class="lineno">  140</span>&#160;            <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; <a class="code" href="class_adagrad_optimizer.html#a4d44b0edfcacb2b53c137a364a22eeca">m_numOfParameter</a>; i++) {</div><div class="line"><a name="l00141"></a><span class="lineno">  141</span>&#160;                <span class="keywordflow">if</span> ((*<a class="code" href="class_adagrad_optimizer.html#a693d28d980ee2772a62006137e96f4bd">m_ppParameter</a>)[i]-&gt;GetIsTrainable()) <a class="code" href="class_adagrad_optimizer.html#a770179d3d869898282bb7397e93003fb">UpdateParameter</a>((*<a class="code" href="class_adagrad_optimizer.html#a693d28d980ee2772a62006137e96f4bd">m_ppParameter</a>)[i]);</div><div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;            }</div><div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;        } <span class="keywordflow">else</span> {</div><div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;            <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; <a class="code" href="class_adagrad_optimizer.html#a4d44b0edfcacb2b53c137a364a22eeca">m_numOfParameter</a>; i++) {</div><div class="line"><a name="l00145"></a><span class="lineno">  145</span>&#160;                <span class="keywordflow">if</span> ((*<a class="code" href="class_adagrad_optimizer.html#a693d28d980ee2772a62006137e96f4bd">m_ppParameter</a>)[i]-&gt;GetIsTrainable()) <a class="code" href="class_adagrad_optimizer.html#a770179d3d869898282bb7397e93003fb">UpdateParameter</a>((*<a class="code" href="class_adagrad_optimizer.html#a693d28d980ee2772a62006137e96f4bd">m_ppParameter</a>)[i], (*<a class="code" href="class_adagrad_optimizer.html#a99aa9af52b00e6b72774d9e602b9cfbc">m_aaGradientSquared</a>)[i]);</div><div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;            }</div><div class="line"><a name="l00147"></a><span class="lineno">  147</span>&#160;        }</div><div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;        <span class="keywordflow">return</span> TRUE;</div><div class="line"><a name="l00149"></a><span class="lineno">  149</span>&#160;    }</div><div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;</div><div class="line"><a name="l00156"></a><span class="lineno"><a class="line" href="class_adagrad_optimizer.html#a60131cf4400fbc376bb584a6652f1723">  156</a></span>&#160;    <span class="keywordtype">int</span> <a class="code" href="class_adagrad_optimizer.html#a60131cf4400fbc376bb584a6652f1723">UpdateParameter</a>(<a class="code" href="class_operator.html">Operator&lt;DTYPE&gt;</a> *pParameter) {</div><div class="line"><a name="l00157"></a><span class="lineno">  157</span>&#160;        <a class="code" href="class_tensor.html">Tensor&lt;DTYPE&gt;</a> *trainable_data = pParameter-&gt;GetResult();</div><div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;        <a class="code" href="class_tensor.html">Tensor&lt;DTYPE&gt;</a> *gradient       = pParameter-&gt;GetGradient();</div><div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;</div><div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;        <span class="keywordtype">float</span> learning_rate = this-&gt;GetOptimizeDirection() * this-&gt;GetLearningRate();</div><div class="line"><a name="l00161"></a><span class="lineno">  161</span>&#160;</div><div class="line"><a name="l00162"></a><span class="lineno">  162</span>&#160;        <span class="keywordtype">int</span> capacity = trainable_data-&gt;<a class="code" href="class_tensor.html#ac9fe35d8f9056cdd8d3a802a85678f4c">GetCapacity</a>();</div><div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;</div><div class="line"><a name="l00164"></a><span class="lineno">  164</span>&#160;        <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; capacity; i++) {</div><div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;            (*trainable_data)[i] += learning_rate * (*gradient)[i];</div><div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;        }</div><div class="line"><a name="l00167"></a><span class="lineno">  167</span>&#160;</div><div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;        <span class="keywordflow">return</span> TRUE;</div><div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;    }</div><div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;</div><div class="line"><a name="l00178"></a><span class="lineno"><a class="line" href="class_adagrad_optimizer.html#af728a868bb4faf9885d9b2707ebd04ef">  178</a></span>&#160;    <span class="keywordtype">int</span> <a class="code" href="class_adagrad_optimizer.html#af728a868bb4faf9885d9b2707ebd04ef">UpdateParameter</a>(<a class="code" href="class_operator.html">Operator&lt;DTYPE&gt;</a> *pParameter, <a class="code" href="class_tensor.html">Tensor&lt;DTYPE&gt;</a> *m_pGradientSquared) {</div><div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160;        <a class="code" href="class_tensor.html">Tensor&lt;DTYPE&gt;</a> *trainable_data = pParameter-&gt;GetResult();</div><div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160;        <a class="code" href="class_tensor.html">Tensor&lt;DTYPE&gt;</a> *gradient       = pParameter-&gt;GetGradient();</div><div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160;</div><div class="line"><a name="l00182"></a><span class="lineno">  182</span>&#160;        <span class="keywordtype">float</span> signed_learning_rate = this-&gt;GetOptimizeDirection() * this-&gt;GetLearningRate();</div><div class="line"><a name="l00183"></a><span class="lineno">  183</span>&#160;</div><div class="line"><a name="l00184"></a><span class="lineno">  184</span>&#160;        <span class="keywordtype">int</span> capacity = trainable_data-&gt;<a class="code" href="class_tensor.html#ac9fe35d8f9056cdd8d3a802a85678f4c">GetCapacity</a>();</div><div class="line"><a name="l00185"></a><span class="lineno">  185</span>&#160;</div><div class="line"><a name="l00186"></a><span class="lineno">  186</span>&#160;        <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; capacity; i++) {</div><div class="line"><a name="l00187"></a><span class="lineno">  187</span>&#160;            (*m_pGradientSquared)[i] = ((*gradient)[i] * (*gradient)[i]);</div><div class="line"><a name="l00188"></a><span class="lineno">  188</span>&#160;            (*trainable_data)[i]    += (signed_learning_rate * (*gradient)[i]) / std::sqrt((*m_pGradientSquared)[i] + <a class="code" href="class_adagrad_optimizer.html#a24a936788c47e2f08a72d6862a312798">m_epsilon</a>);</div><div class="line"><a name="l00189"></a><span class="lineno">  189</span>&#160;        }</div><div class="line"><a name="l00190"></a><span class="lineno">  190</span>&#160;</div><div class="line"><a name="l00191"></a><span class="lineno">  191</span>&#160;        <span class="keywordflow">return</span> TRUE;</div><div class="line"><a name="l00192"></a><span class="lineno">  192</span>&#160;    }</div><div class="line"><a name="l00193"></a><span class="lineno">  193</span>&#160;</div><div class="line"><a name="l00194"></a><span class="lineno">  194</span>&#160;<span class="preprocessor">#ifdef __CUDNN__</span></div><div class="line"><a name="l00195"></a><span class="lineno">  195</span>&#160;</div><div class="line"><a name="l00202"></a><span class="lineno">  202</span>&#160;    <span class="keyword">virtual</span> <span class="keywordtype">int</span> UpdateParameterOnGPU() {</div><div class="line"><a name="l00203"></a><span class="lineno">  203</span>&#160;        <span class="keywordflow">if</span> (<a class="code" href="class_adagrad_optimizer.html#a24a936788c47e2f08a72d6862a312798">m_epsilon</a> == 0.f) {</div><div class="line"><a name="l00204"></a><span class="lineno">  204</span>&#160;            <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; <a class="code" href="class_adagrad_optimizer.html#a4d44b0edfcacb2b53c137a364a22eeca">m_numOfParameter</a>; i++) {</div><div class="line"><a name="l00205"></a><span class="lineno">  205</span>&#160;                <span class="keywordflow">if</span> ((*<a class="code" href="class_adagrad_optimizer.html#a693d28d980ee2772a62006137e96f4bd">m_ppParameter</a>)[i]-&gt;GetIsTrainable()) UpdateParameterOnGPU((*<a class="code" href="class_adagrad_optimizer.html#a693d28d980ee2772a62006137e96f4bd">m_ppParameter</a>)[i]);</div><div class="line"><a name="l00206"></a><span class="lineno">  206</span>&#160;            }</div><div class="line"><a name="l00207"></a><span class="lineno">  207</span>&#160;        } <span class="keywordflow">else</span> {</div><div class="line"><a name="l00208"></a><span class="lineno">  208</span>&#160;            <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; <a class="code" href="class_adagrad_optimizer.html#a4d44b0edfcacb2b53c137a364a22eeca">m_numOfParameter</a>; i++) {</div><div class="line"><a name="l00209"></a><span class="lineno">  209</span>&#160;                <span class="keywordflow">if</span> ((*<a class="code" href="class_adagrad_optimizer.html#a693d28d980ee2772a62006137e96f4bd">m_ppParameter</a>)[i]-&gt;GetIsTrainable()) UpdateParameterOnGPU((*<a class="code" href="class_adagrad_optimizer.html#a693d28d980ee2772a62006137e96f4bd">m_ppParameter</a>)[i], (*<a class="code" href="class_adagrad_optimizer.html#a99aa9af52b00e6b72774d9e602b9cfbc">m_aaGradientSquared</a>)[i]);</div><div class="line"><a name="l00210"></a><span class="lineno">  210</span>&#160;            }</div><div class="line"><a name="l00211"></a><span class="lineno">  211</span>&#160;        }</div><div class="line"><a name="l00212"></a><span class="lineno">  212</span>&#160;        <span class="keywordflow">return</span> TRUE;</div><div class="line"><a name="l00213"></a><span class="lineno">  213</span>&#160;    }</div><div class="line"><a name="l00214"></a><span class="lineno">  214</span>&#160;</div><div class="line"><a name="l00220"></a><span class="lineno">  220</span>&#160;    <span class="keywordtype">int</span> UpdateParameterOnGPU(<a class="code" href="class_operator.html">Operator&lt;DTYPE&gt;</a> *pParameter) {</div><div class="line"><a name="l00221"></a><span class="lineno">  221</span>&#160;        <span class="comment">// Tensor&lt;DTYPE&gt; *trainable_data = pParameter-&gt;GetResult();</span></div><div class="line"><a name="l00222"></a><span class="lineno">  222</span>&#160;        <span class="comment">// Tensor&lt;DTYPE&gt; *gradient       = pParameter-&gt;GetGradient();</span></div><div class="line"><a name="l00223"></a><span class="lineno">  223</span>&#160;        <span class="comment">//</span></div><div class="line"><a name="l00224"></a><span class="lineno">  224</span>&#160;        <span class="comment">// cudnnTensorDescriptor_t dataDesc = trainable_data-&gt;GetDescriptor();</span></div><div class="line"><a name="l00225"></a><span class="lineno">  225</span>&#160;        <span class="comment">// cudnnTensorDescriptor_t gradDesc = gradient-&gt;GetDescriptor();</span></div><div class="line"><a name="l00226"></a><span class="lineno">  226</span>&#160;        <span class="comment">//</span></div><div class="line"><a name="l00227"></a><span class="lineno">  227</span>&#160;        <span class="comment">// DTYPE *m_pDevData = trainable_data-&gt;GetGPUData();</span></div><div class="line"><a name="l00228"></a><span class="lineno">  228</span>&#160;        <span class="comment">// DTYPE *m_pDevGrad = gradient-&gt;GetGPUData();</span></div><div class="line"><a name="l00229"></a><span class="lineno">  229</span>&#160;        <span class="comment">//</span></div><div class="line"><a name="l00230"></a><span class="lineno">  230</span>&#160;        <span class="comment">// float learning_rate = this-&gt;GetOptimizeDirection() * this-&gt;GetLearningRate();</span></div><div class="line"><a name="l00231"></a><span class="lineno">  231</span>&#160;        <span class="comment">//</span></div><div class="line"><a name="l00232"></a><span class="lineno">  232</span>&#160;        <span class="comment">// float alpha = 1.f;</span></div><div class="line"><a name="l00233"></a><span class="lineno">  233</span>&#160;        <span class="comment">// float beta  = learning_rate;</span></div><div class="line"><a name="l00234"></a><span class="lineno">  234</span>&#160;        <span class="comment">//</span></div><div class="line"><a name="l00235"></a><span class="lineno">  235</span>&#160;        <span class="comment">// checkCUDNN(cudnnAddTensor(this-&gt;GetCudnnHandle(),</span></div><div class="line"><a name="l00236"></a><span class="lineno">  236</span>&#160;        <span class="comment">// &amp;beta, gradDesc, m_pDevGrad,</span></div><div class="line"><a name="l00237"></a><span class="lineno">  237</span>&#160;        <span class="comment">// &amp;alpha, dataDesc, m_pDevData));</span></div><div class="line"><a name="l00238"></a><span class="lineno">  238</span>&#160;</div><div class="line"><a name="l00239"></a><span class="lineno">  239</span>&#160;        <span class="keywordflow">return</span> TRUE;</div><div class="line"><a name="l00240"></a><span class="lineno">  240</span>&#160;    }</div><div class="line"><a name="l00241"></a><span class="lineno">  241</span>&#160;</div><div class="line"><a name="l00242"></a><span class="lineno">  242</span>&#160;    <span class="keywordtype">int</span> UpdateParameterOnGPU(<a class="code" href="class_operator.html">Operator&lt;DTYPE&gt;</a> *pParameter, <a class="code" href="class_tensor.html">Tensor&lt;DTYPE&gt;</a> *pGradientSquared);</div><div class="line"><a name="l00243"></a><span class="lineno">  243</span>&#160;</div><div class="line"><a name="l00244"></a><span class="lineno">  244</span>&#160;    <span class="comment">// __global__ void rsquare(DTYPE *myArrayGPU) {</span></div><div class="line"><a name="l00245"></a><span class="lineno">  245</span>&#160;    <span class="comment">// myArrayGPU[capacity] = pow((float) capacity, -0.5);</span></div><div class="line"><a name="l00246"></a><span class="lineno">  246</span>&#160;    <span class="comment">// }</span></div><div class="line"><a name="l00247"></a><span class="lineno">  247</span>&#160;</div><div class="line"><a name="l00248"></a><span class="lineno">  248</span>&#160;<span class="preprocessor">#endif  // if __CUDNN__</span></div><div class="line"><a name="l00249"></a><span class="lineno">  249</span>&#160;};</div><div class="line"><a name="l00250"></a><span class="lineno">  250</span>&#160;</div><div class="line"><a name="l00251"></a><span class="lineno">  251</span>&#160;</div><div class="line"><a name="l00252"></a><span class="lineno">  252</span>&#160;<span class="preprocessor">#endif  // ADAGRADOPTIMIZER_H_</span></div><div class="ttc" id="class_adagrad_optimizer_html_a4d44b0edfcacb2b53c137a364a22eeca"><div class="ttname"><a href="class_adagrad_optimizer.html#a4d44b0edfcacb2b53c137a364a22eeca">AdagradOptimizer::m_numOfParameter</a></div><div class="ttdeci">int m_numOfParameter</div><div class="ttdoc">업데이트 할 Tensor의 degree </div><div class="ttdef"><b>Definition:</b> <a href="_adagrad_optimizer_8hpp_source.html#l00014">AdagradOptimizer.hpp:14</a></div></div>
<div class="ttc" id="class_adagrad_optimizer_html_a60131cf4400fbc376bb584a6652f1723"><div class="ttname"><a href="class_adagrad_optimizer.html#a60131cf4400fbc376bb584a6652f1723">AdagradOptimizer::UpdateParameter</a></div><div class="ttdeci">int UpdateParameter(Operator&lt; DTYPE &gt; *pParameter)</div><div class="ttdoc">UpdateParameter default 함수 </div><div class="ttdef"><b>Definition:</b> <a href="_adagrad_optimizer_8hpp_source.html#l00156">AdagradOptimizer.hpp:156</a></div></div>
<div class="ttc" id="class_container_html"><div class="ttname"><a href="class_container.html">Container</a></div><div class="ttdef"><b>Definition:</b> <a href="_container_8hpp_source.html#l00012">Container.hpp:12</a></div></div>
<div class="ttc" id="class_adagrad_optimizer_html_ab0087e92268ee3700a50f7e545fb5048"><div class="ttname"><a href="class_adagrad_optimizer.html#ab0087e92268ee3700a50f7e545fb5048">AdagradOptimizer::AdagradOptimizer</a></div><div class="ttdeci">AdagradOptimizer(Container&lt; Operator&lt; DTYPE &gt; *&gt; *pParameterContainer, float pLearningRate, OptimizeDirection pOptimizeDirection)</div><div class="ttdoc">AdagradOptimizer 생성자. </div><div class="ttdef"><b>Definition:</b> <a href="_adagrad_optimizer_8hpp_source.html#l00030">AdagradOptimizer.hpp:30</a></div></div>
<div class="ttc" id="class_adagrad_optimizer_html_ae8b0d226a29ac5716fbffe0add5000ed"><div class="ttname"><a href="class_adagrad_optimizer.html#ae8b0d226a29ac5716fbffe0add5000ed">AdagradOptimizer::Alloc</a></div><div class="ttdeci">int Alloc(float epsilon)</div><div class="ttdoc">Optimizer의 Alloc 매소드 </div><div class="ttdef"><b>Definition:</b> <a href="_adagrad_optimizer_8hpp_source.html#l00106">AdagradOptimizer.hpp:106</a></div></div>
<div class="ttc" id="class_optimizer_html"><div class="ttname"><a href="class_optimizer.html">Optimizer</a></div><div class="ttdef"><b>Definition:</b> <a href="_optimizer_8hpp_source.html#l00011">Optimizer.hpp:11</a></div></div>
<div class="ttc" id="class_container_html_ac875fe061a6d4c1137def5251eaef95a"><div class="ttname"><a href="class_container.html#ac875fe061a6d4c1137def5251eaef95a">Container::Push</a></div><div class="ttdeci">int Push(DTYPE pElement)</div><div class="ttdoc">Queue의 push 메소드 </div><div class="ttdef"><b>Definition:</b> <a href="_container_8hpp_source.html#l00051">Container.hpp:51</a></div></div>
<div class="ttc" id="class_adagrad_optimizer_html_a693d28d980ee2772a62006137e96f4bd"><div class="ttname"><a href="class_adagrad_optimizer.html#a693d28d980ee2772a62006137e96f4bd">AdagradOptimizer::m_ppParameter</a></div><div class="ttdeci">Container&lt; Operator&lt; DTYPE &gt; * &gt; * m_ppParameter</div><div class="ttdoc">값을 업데이트 할 Tensor들을 가리키는 포인터 </div><div class="ttdef"><b>Definition:</b> <a href="_adagrad_optimizer_8hpp_source.html#l00009">AdagradOptimizer.hpp:9</a></div></div>
<div class="ttc" id="class_adagrad_optimizer_html_a770179d3d869898282bb7397e93003fb"><div class="ttname"><a href="class_adagrad_optimizer.html#a770179d3d869898282bb7397e93003fb">AdagradOptimizer::UpdateParameter</a></div><div class="ttdeci">virtual int UpdateParameter()</div><div class="ttdoc">파라미터 값들을 업데이트 하는 메소드 </div><div class="ttdef"><b>Definition:</b> <a href="_adagrad_optimizer_8hpp_source.html#l00138">AdagradOptimizer.hpp:138</a></div></div>
<div class="ttc" id="class_tensor_html"><div class="ttname"><a href="class_tensor.html">Tensor</a></div><div class="ttdef"><b>Definition:</b> <a href="_container_8hpp_source.html#l00003">Container.hpp:3</a></div></div>
<div class="ttc" id="class_tensor_html_ac9fe35d8f9056cdd8d3a802a85678f4c"><div class="ttname"><a href="class_tensor.html#ac9fe35d8f9056cdd8d3a802a85678f4c">Tensor::GetCapacity</a></div><div class="ttdeci">int GetCapacity()</div><div class="ttdoc">Tensor의 m_aLongArray의 Capacity를 반환. </div><div class="ttdef"><b>Definition:</b> <a href="_tensor_8hpp_source.html#l00425">Tensor.hpp:425</a></div></div>
<div class="ttc" id="class_shape_html"><div class="ttname"><a href="class_shape.html">Shape</a></div><div class="ttdef"><b>Definition:</b> <a href="_shape_8hpp_source.html#l00014">Shape.hpp:14</a></div></div>
<div class="ttc" id="class_adagrad_optimizer_html_aa4c51cf728ca90d2aa6bc71d1c4f22d6"><div class="ttname"><a href="class_adagrad_optimizer.html#aa4c51cf728ca90d2aa6bc71d1c4f22d6">AdagradOptimizer::Delete</a></div><div class="ttdeci">void Delete()</div><div class="ttdoc">Optimizer의 Delete 매소드 </div><div class="ttdef"><b>Definition:</b> <a href="_adagrad_optimizer_8hpp_source.html#l00083">AdagradOptimizer.hpp:83</a></div></div>
<div class="ttc" id="class_adagrad_optimizer_html_a4eece73c46a11c319a93ca89a589d730"><div class="ttname"><a href="class_adagrad_optimizer.html#a4eece73c46a11c319a93ca89a589d730">AdagradOptimizer::AdagradOptimizer</a></div><div class="ttdeci">AdagradOptimizer(Container&lt; Operator&lt; DTYPE &gt; *&gt; *pParameterContainer, float pLearningRate, float epsilon, OptimizeDirection pOptimizeDirection)</div><div class="ttdoc">AdagradOptimizer 생성자. </div><div class="ttdef"><b>Definition:</b> <a href="_adagrad_optimizer_8hpp_source.html#l00054">AdagradOptimizer.hpp:54</a></div></div>
<div class="ttc" id="class_adagrad_optimizer_html_a24a936788c47e2f08a72d6862a312798"><div class="ttname"><a href="class_adagrad_optimizer.html#a24a936788c47e2f08a72d6862a312798">AdagradOptimizer::m_epsilon</a></div><div class="ttdeci">float m_epsilon</div><div class="ttdoc">분모 값이 0이 되는 것을 방지 하는 값 </div><div class="ttdef"><b>Definition:</b> <a href="_adagrad_optimizer_8hpp_source.html#l00016">AdagradOptimizer.hpp:16</a></div></div>
<div class="ttc" id="class_adagrad_optimizer_html_af728a868bb4faf9885d9b2707ebd04ef"><div class="ttname"><a href="class_adagrad_optimizer.html#af728a868bb4faf9885d9b2707ebd04ef">AdagradOptimizer::UpdateParameter</a></div><div class="ttdeci">int UpdateParameter(Operator&lt; DTYPE &gt; *pParameter, Tensor&lt; DTYPE &gt; *m_pGradientSquared)</div><div class="ttdoc">파라미터 값들을 업데이트 하는 메소드 </div><div class="ttdef"><b>Definition:</b> <a href="_adagrad_optimizer_8hpp_source.html#l00178">AdagradOptimizer.hpp:178</a></div></div>
<div class="ttc" id="class_adagrad_optimizer_html"><div class="ttname"><a href="class_adagrad_optimizer.html">AdagradOptimizer</a></div><div class="ttdef"><b>Definition:</b> <a href="_adagrad_optimizer_8hpp_source.html#l00007">AdagradOptimizer.hpp:7</a></div></div>
<div class="ttc" id="class_operator_html"><div class="ttname"><a href="class_operator.html">Operator</a></div><div class="ttdef"><b>Definition:</b> <a href="_container_8hpp_source.html#l00004">Container.hpp:4</a></div></div>
<div class="ttc" id="class_adagrad_optimizer_html_a9b28f6ca7f58bd151f44e5043d2735b2"><div class="ttname"><a href="class_adagrad_optimizer.html#a9b28f6ca7f58bd151f44e5043d2735b2">AdagradOptimizer::~AdagradOptimizer</a></div><div class="ttdeci">~AdagradOptimizer()</div><div class="ttdoc">AdagradOptimizer 소멸자 </div><div class="ttdef"><b>Definition:</b> <a href="_adagrad_optimizer_8hpp_source.html#l00071">AdagradOptimizer.hpp:71</a></div></div>
<div class="ttc" id="class_adagrad_optimizer_html_a99aa9af52b00e6b72774d9e602b9cfbc"><div class="ttname"><a href="class_adagrad_optimizer.html#a99aa9af52b00e6b72774d9e602b9cfbc">AdagradOptimizer::m_aaGradientSquared</a></div><div class="ttdeci">Container&lt; Tensor&lt; DTYPE &gt; * &gt; * m_aaGradientSquared</div><div class="ttdoc">의 제곱으로 업데이트 되는 variable </div><div class="ttdef"><b>Definition:</b> <a href="_adagrad_optimizer_8hpp_source.html#l00011">AdagradOptimizer.hpp:11</a></div></div>
</div><!-- fragment --></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="dir_d1e2cb908f5411cf8e396afb908a20dd.html">WICWIU_src</a></li><li class="navelem"><a class="el" href="dir_bb6e272ecaeb72399e574062e3c55fe2.html">Optimizer</a></li><li class="navelem"><b>AdagradOptimizer.hpp</b></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.14 </li>
  </ul>
</div>
</body>
</html>
